
<!DOCTYPE html>
<html lang="en">
<head>
    <title> Bram Banik</title>
    <link rel =  "stylesheet" type = "text/css" href = "robotSummerStyle.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&family=Roboto:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
    <meta charset="UTF-8">
    <meta name = "viewport"  content = "width = device-width, initial-scale=1">
    <style>
        body {
            font-family: 'Roboto', sans-serif;
        }
    </style>
</head>

<body style = "margin:0"> 
    <div id="header"></div>
    <article>
        <div class = "main_image">
            <img src="/media/robot_header.png" alt = "Robot Summer Image">
        </div>
        <h1>ENPH253 Robot Summer: Big Dawg</h1>
        <h2>Sunday, August 10, 2025</h2>
        <hr>
        <p>
            Many Engineering Physics students look forward to the summer of their second year in university, and for good reason. 
            It marks the beginning of a rigorous yet friendly competition coloquially known as "Robot Summer."
            Teams of four compete to design a robot that autonomously navigates a course that changes every year. This 
            year, the theme was Pet Rescue! Pets (represented by beanie babies) are trapped in a burning building and it's our 
            job to save them!
        </p>
        <h3>Tackling the Design Challenge</h3>
        <p>Our team quickly realized there was a lot more to rescuing a pet than we had initially imagined. Not only did the robot have to drive along a line, it also had to
            be able to concurrently detect pets, determine distances, relay information to the claw, and of course actually pick up pets. This requires integration of the three 
            main branches of Engineering Physics: Software, Mechanical, and Electrical engineering. I was delegated to the majority of electrical and software work. As such, my first
            project was figuring out the circuit to drive the motors, which introduced my long debugging battle with one of the most infamous circuits: the H-Bridge.
        </p>    
        <h3>H-Bridges: Ever Useful, Notoriously Finicky</h3>
        <p>One of the most important aspects of an autonomous robot is the driving system. Without it, the robot can't really
            do anything of practical use. While it's possible that you could wire up a MOSFET with a PWM to the gate and change the speed
            of a motor, this only allows unidirectional rotation unless you physically flip the voltage leads. That's where the H-Bridge comes in:
            it allows you to spin a motor in both directions through PWM. 
        </p>
        <div class = "hbridge_schematic">
            <img src = "/media/hbridge_schematic.png" alt = "H-Bridge Schematic">
            <h4>Fig. 1: H-Bridge Schematic</h4>
        </div>
        <p>This was my first real foray into circuit design and using software like KiCad. The circuit shown here was designed by one of our instructors,
            which I drew up in KiCad. To make our robot cleaner and easier to debug, we decided to order PCBs, which meant I had to design 
            custom boards for all of our circuits. Shown below is the PCB design we ended up with, after a couple of revisions. One of the key aspects of this design
            is the spike-filtering capacitors and larger traces going to the motors. These are both to ensure smooth operation of the H-Bridge and 
            to avoid any possible blown wires.
        </p>
        <div class = "pcb_images">
            <img src = "/media/pcb_schematic.png" alt = "H-Bridge PCB">
            <h4>Fig. 2: H-Bridge PCB Schematic</h4>
            <img src = "/media/pcb_model.png" alt = "H-Bridge Model">
            <h4>Fig. 3: H-Bridge PCB Model</h4>
        </div>
        <p>So, I soldered up the H-Bridge and tested it, and it worked on the first try! Is what I would have said if I hadn't faced multiple weeks of debugging and redesign.
            As it stood, only the top H-Bridge worked on my dual-H-Bridge PCB, and even then it was finicky. I would encounter shoot-through, noise, and temporary operation until 
            it would randomly fail. This obviously made testing other parts of our robot incredibly difficult. I will spare you the 2+ weeks of debugging multiple hours a day, but 
            I assure you I would check for shorts in my soldering, ensure inputs were properly connected, etc...
            My best guess after many painful hours would be some sort of design flaw in the PCB itself - although the schematic was correct, there are portions
            of the ground plane and +12V plane that are connected by only thin vias. While on the surface this shouldn't matter too much (they are electrically connected, after all), I believe it may have 
            some significant implications.
        </p>
        <p>
            I think this is a good time to mention that this dual H-Bridge design is somewhat out of spec. Rather than using two PWM signals, it uses a single PWM and a direction pin at logic HIGH or LOW.
            It abuses the built-in timer pins on our gate driver, the LT1161, by triggering the over-current detector and telling the gate driver to turn off one half of the bridge.
            These timer pins are linked to the sense pins, which trigger a shut-off if the sense pin drops 65mV below the positive voltage supply. The fact that the +12V plane is occasionally
            connected by what is effectively a wire definitely leaves an avenue for voltage differences across the board's planes. However, I can't be sure. Due to this inconsistent behavior,
            I ended up scrapping this design and switching to a simpler, more traditional dual-PWM input H-Bridge design, shown below. Lo and behold, much to my relief, these generally worked
            right after soldering, albeit they do not have the professional luster that comes with manufactured PCBs.
            
        </p>
        <div class = "new_hbridge">
            <img src = "/media/new_bridge.jpg" alt = "New Dual-PWM H-Bridge">
        </div>
        <h4>Fig. 4: Final H-Bridge Board</h4>
        <h3>Giving the Robot Eyes</h3>
        <div class = "esp32_image">
          <img src = "/media/esp32.png" alt = "ESP32" class="esp32_image">
          <h5>ESP32</h5>
        </div>
        <p>
            One of the most important aspects of an autonomous robot is its control system. You can craft the most mechanically elegant robot, but it's practically 
            paperweight if you don't implement a proper algorithm. We knew we would control our robot using the ESP32
            Pico Kit 1 (as it was the one we were given). However, we soon ran into our first major design decision: how were we going to detect pets? 
            The competition organizers had sewn small magnets into the heads of each pet with the explicit intent for detection, but this quickly proved infeasable. 
            The magnetometers were only capable of detecting up to 2 inches away, meaning we would have to practically sweep the entire competition surface. While other 
            teams switched to time-of-flight LIDAR systems or infrared detection, we ended up settling on a more advanced solution: computer vision.
        </p>
        <p>
            This immediately had a lot of important implications. First and foremost, the ESP32 was not nearly powerful enough to run an entire computer vision model on top 
            of other robot functions. We settled on mounting a Raspberry Pi 5 with an attached Pi camera to handle the detection. To communicate between the two boards,
            we used the Serial TX and RX ports, specifically Serial Port 0. If you've worked with ESP32s before, you might recognize an obvious problem with this: uploading code 
            utilizes this port. Our simple solution was to disconnect the Pi each time we uploaded code - annoying, yet effective.
        </p>
        <p>
            Training the computer vision model proved to not be the arduous task we had imagined. One of my teammates handled the majority of the CV setup, with some teamwork along the 
            way to troubleshoot ESP32 communication problems. The longest part was gathering data: we ended up with over 4,000 images of stuffed pets 
            with different backgrounds, lightings, and orientations. The end results, after training a custom YOLO CV model, are shown below. 
        </p>

        <div>
            <h4>Fig. 5: Computer Vision Detection</h4>
        </div>
        <p>
            Due to hardware limitations on the Raspberry Pi 5, we were only able to reach about 5 FPS for detection, but this 
            was more than enough given the speed of our robot (~0.3 meters per second). While an AI Hat like one of the Hailo models 
            would have supercharged our performance, that would put us way above the set $200 budget for competition.
        </p>
        <h3>Control System: Foray into freeRTOS</h3>
        <p>With detection out of the way, we had to start actually getting the robot to function. The most obvious way to autonomously control the robot 
            would be through a "superloop." As the name suggests, this consists of one large loop() function that would run all of the robot's tasks: driving, detection,
            pickup, etc... However, this has some not-so-obvious drawbacks. One of the primary functions of a robot is tape-following, which uses a PID (proportional-integral-derivative) loop to 
            correct itself onto a line. This requires a low-latency response from detecting the line's position relative to the robot and actuating the motors accordingly. Let's say, for accurate line
            following, you need at most a 10ms delay between each line check. In a superloop, you now need to guarantee that all of the other robot functions can occur within 10ms! 
            If you happen to have a detection algorithm that runs slowly, you then risk sending your robot off the track. This usually means a failed run. 
        </p>
        <p> So how do we avoid this? Well, we could hyper-optimize our code with flags and checking time stamps to ensure a fast response time, but that is a lot of work. Luckily, there is 
            another way that simplifies this process: freeRTOS. This is a built-in library to the ESP32 that allows you to simulate parallelism through time-stitching functions. The ESP32 only 
            has one core, so it's necessary to use time-stitching rather than running multiple threads. It also introduced a new structure to our code known as tasks. Essentially, we create 
            tasks that support key functions of the robot, which then can communicate with each other and run concurrently. For example, here is what a driving task may look like:
        </p>
        <div class = "code_block">
<pre><code>void drive_task(void *parameters)  {

        ulTaskNotifyTake(pdTRUE, portMAX_DELAY);
    
    for (;;) {
        robot->drivePID(speed);
        if (run && millis() - startTime > driveBackTime) {
            vTaskDelete(NULL);
        }
        vTaskDelay(pdMS_TO_TICKS(2));
    }
} </code></pre>
        </div>

        <p>
            This little snippet has a couple important lines of code. The <code>ulTaskNotifyTake</code> essentially tells the task to wait until it has been 
            notified to start elsewhere. After the task has begun, it starts an unending loop that tells the robot to drive with PID control. In a superloop, this would be problematic  
            as no other code could run during this for loop. However, freeRTOS automatically partitions CPU time and allows other tasks to run concurrently! 
            Of course, these tasks also need to be intialized in the setup() function, and follow the following format:
        </p>

        <div class = "code_block">
        <pre><code>xTaskCreate(
drive_task,   // function to be run
"Driving",    // description of task
4096,         // bytes allocated to this 
NULL,         // parameters, dependent on function
1,            // priority
&drive_handle // task handle
);  </code></pre>
        </div>
        <p>
            Another key component of tasks are their priority and the task handle. The priority indicates to the CPU how important the task is - a lower 
            priority task will not run if a higher priority task is already scheduled. This is helpful for mission-critical tasks like preventing the robot 
            from falling of a ledge. The task handle is how you communicate between tasks. Remember the notification system mentioned earlier? The task handles 
            allow you to send notifications to specific tasks.
        </p>
        <p>
            The final thing I'd like to touch on is how we structured our codebase. If you visit our <a href = "https://github.com/justinqian1/enph253_team8_software">codebase on GitHub</a>, 
            you'll notice that each hardware component has its own object. This was done to decrease clutter in our main function (although there is still a lot of commented code there). 
            Furthermore, the claw and the driving wheels have their own classes as well, which take pointers to our hardware objects as inputs. This 
            allows for more elegant coding. Check out the difference between these two snippets:
        </p>
        <div class = "code_block">
        <pre><code>loop() {
    last_distance=distance;
    distance = distToTape();
    
    if(last_distance != distance) {
        q=m;
        m=1;
    }

    adc2_get_raw(ADC2_CHANNEL_7, ADC_WIDTH_10Bit, &kp); 
    adc2_get_raw(ADC2_CHANNEL_6, ADC_WIDTH_10Bit, &kd);
    p=kp*distance;
    d=(int) ((float)kd*(float)(distance - last_distance)/(float)(q+m));
    //i+=ki*distance;
    ctrl=(int)(p+d); 
    m++; 
    
    //digitalWrite(dirOut1, dir1);
    //digitalWrite(dirOut2, dir2);
    leftSpeed = max(speed-ctrl,minSpeed);
    leftSpeed = min(leftSpeed,maxSpeed);
    rightSpeed = max(speed+ctrl,minSpeed);
    rightSpeed = min(rightSpeed,maxSpeed);
    ledcWrite(leftPwmChannel,leftSpeed);
    ledcWrite(rightPwmChannel,rightSpeed);

    leftVal = adc1_get_raw(ADC1_CHANNEL_4);
    rightVal = adc1_get_raw(ADC1_CHANNEL_5);

    delay(2); 
}   </code></pre>
        </div>
         <div class = "code_block">
        <pre>  <code>loop() {

    robot->drivePID(speed);

    delay(2);
}     </code></pre>
        </div>
        <p>
            Much cleaner and more compartmentalized, making it significantly easier to debug!
        </p>

        <h3>Our Robot: Big Dawg</h3>
        <div class = "video">
            <video height = "480" playsinline autoplay loop muted>
                <source src="/media/IMG_1543.mp4" type="video/mp4"/>
            </video>
        </div>
        <p>After many late hours in the lab, lots of blown ICs, and plenty of headaches, we managed to build a robot 
            we were proud of. The name came quickly: Big Dawg, because who doesn't like a big dog. Although we didn't performan 
            the best during the competition, it was a real joy to see all of our hard work integrated. I've attached a video clip 
            below of our robot detecting and picking up a pet autonomously, recorded at 4AM the morning of competition. It's honestly
            amazing to see our robot do it's thing, driving around and picking up pets. This course has taught me the most applicable skills 
            so far in university, and I can't wait to bring these skills to future projects. To follow up with what I've learned here, I've 
            decided to focus my next efforts on building a stronger foundation in computer vision. I've order a Jetson Orin Nano and I'm excited 
            to test out more applications of Edge AI.
        </p>
        
        <p>
            Thank you for reading to the end! Feel free to contact me if you have any questions or comments at <a href="mailto:brambanik@gmail.com">brambanik@gmail.com</a>!
        </p>
    </article>

    <script>
        fetch("/header.html")
            .then(res => res.text())
            .then(data => document.getElementById("header").innerHTML = data);
    </script>
</body>
